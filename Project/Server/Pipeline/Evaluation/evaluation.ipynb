{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6c3d673-2b4b-4b78-939d-4b1fcfb9f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import randrange\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40df4a00-61de-4d1a-8a0f-a114b014d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/lyrics.json', 'r', encoding='utf8') as f:\n",
    "    dataset = json.load(f)\n",
    "       \n",
    "import pandas as pd\n",
    "labeled_songs = pd.read_csv(\"labeled_songs.csv\")\n",
    "    \n",
    "import ast\n",
    "data = []\n",
    "with open(\"songs_to_label.txt\", \"r\", encoding='utf8') as inFile:\n",
    "    data = ast.literal_eval(inFile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19ad5515-d588-4c68-932f-c4f338ebab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining two dictionaries for bidirectional mapping between int & class\n",
    "class_to_int = {'neutral' : 0, 'liebevoll' : 1, 'gewaltt√§tig' : 2, 'rassistisch' : 3,\n",
    "                   'homophob' : 4, 'frauenfeindlich' : 5, 'freundlich' : 6, 'positiv' : 7, 'traurig' : 8}\n",
    "int_to_class = {v: k for k, v in class_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d01240a-e558-4eb1-8753-5387b5dabeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to get the highest scoring class for a song\n",
    "def get_highest_class(song):\n",
    "    max_class = 0\n",
    "    highest_score = 0\n",
    "    for key, value in song['total_class_score'].items():\n",
    "        if highest_score < value:\n",
    "            max_class = class_to_int[key]\n",
    "            highest_score = value\n",
    "    return max_class\n",
    "\n",
    "## find a song in the dataset by its genius_track_id\n",
    "def find_song(dataset, idx):\n",
    "    for song in dataset:\n",
    "        if song['genius_track_id'] == idx:\n",
    "            return song\n",
    "    print(\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e938b91-d6de-4702-bab6-0fbde1fb0bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## randomly sample n songs for each class for a total of 9*n (9 classes)\n",
    "import random\n",
    "\n",
    "total = 0\n",
    "n = 10\n",
    "attempts = 0\n",
    "random.seed(237)\n",
    "\n",
    "samples = []\n",
    "for i in range(9):\n",
    "    elist = []\n",
    "    samples.append(elist)\n",
    "while total < 9*n and attempts < 1000:\n",
    "    idx = random.randrange(len(dataset))\n",
    "    class_number = get_highest_class(dataset[idx])\n",
    "    if (not dataset[idx] in samples[class_number]) and (len(samples[class_number]) < n):\n",
    "        samples[class_number].append(dataset[idx])\n",
    "        total += 1\n",
    "    attempts += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f55cc3c-eb0b-4317-b332-b3fbc7b4a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all the songs, that were manually labeled, into a list for the evaluation\n",
    "## also sets a new attribute 'top_class' for each song for the highest scoring class\n",
    "songs_list = []\n",
    "for idx in labeled_songs['id']:\n",
    "    song = find_song(dataset, idx)\n",
    "    top_class = get_highest_class(song)\n",
    "    song['top_class'] = top_class\n",
    "    songs_list.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3ed8c67-bf61-411c-bb62-eaf19fde2651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## creating the different prediction & groundtruth (our labels) lists.\n",
    "\n",
    "classification_prediction = []\n",
    "sentiment_prediction = []\n",
    "toxicity_prediction = []\n",
    "classification_truth = []\n",
    "sentiment_truth = []\n",
    "toxicity_truth = []\n",
    "\n",
    "for idx, song in enumerate(songs_list):\n",
    "    classification_prediction.append(song['top_class'])\n",
    "    sentiment = song['sentiment_value']\n",
    "    if sentiment < 0:\n",
    "        sentiment = -1\n",
    "    else:\n",
    "        sentiment = 1\n",
    "    sentiment_prediction.append(sentiment)\n",
    "    toxicity = song['toxicity_value']\n",
    "    if toxicity >= 0.5:\n",
    "        toxicity = 1\n",
    "    else:\n",
    "        toxicity = -1\n",
    "    toxicity_prediction.append(toxicity)\n",
    "    classification_truth.append(labeled_songs.iloc[idx]['classifier'])\n",
    "    sentiment_truth.append(labeled_songs.iloc[idx]['sentiment'])\n",
    "    toxicity_truth.append(labeled_songs.iloc[idx]['toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47eebabb-ce2e-4041-9604-8d7076eb5b94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.22      0.22         9\n",
      "           1       0.70      0.78      0.74         9\n",
      "           2       0.67      0.40      0.50        15\n",
      "           3       0.00      0.00      0.00         1\n",
      "           5       0.33      0.60      0.43         5\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.36      0.50      0.42         8\n",
      "           8       1.00      0.48      0.65        21\n",
      "\n",
      "    accuracy                           0.47        68\n",
      "   macro avg       0.41      0.37      0.37        68\n",
      "weighted avg       0.65      0.47      0.52        68\n",
      "\n",
      "sentiment:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.63      0.98      0.77        42\n",
      "           1       0.67      0.08      0.14        26\n",
      "\n",
      "    accuracy                           0.63        68\n",
      "   macro avg       0.65      0.53      0.45        68\n",
      "weighted avg       0.64      0.63      0.53        68\n",
      "\n",
      "toxicity:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.53      1.00      0.69        35\n",
      "           1       1.00      0.06      0.11        33\n",
      "\n",
      "    accuracy                           0.54        68\n",
      "   macro avg       0.77      0.53      0.40        68\n",
      "weighted avg       0.76      0.54      0.41        68\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsindlinger\\Documents\\Uni\\Data Science for Text Analysis\\Project\\Server\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gsindlinger\\Documents\\Uni\\Data Science for Text Analysis\\Project\\Server\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gsindlinger\\Documents\\Uni\\Data Science for Text Analysis\\Project\\Server\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gsindlinger\\Documents\\Uni\\Data Science for Text Analysis\\Project\\Server\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gsindlinger\\Documents\\Uni\\Data Science for Text Analysis\\Project\\Server\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\gsindlinger\\Documents\\Uni\\Data Science for Text Analysis\\Project\\Server\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "## we're interested in the macro-avg\n",
    "print(\"classifier:\")\n",
    "print(classification_report(classification_truth, classification_prediction))\n",
    "print(\"sentiment:\")\n",
    "print(classification_report(sentiment_truth, sentiment_prediction))\n",
    "print(\"toxicity:\")\n",
    "print(classification_report(toxicity_truth, toxicity_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9417ac1-bd79-49e7-9272-364f4fdf5cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7e01cce-15a8-4514-9805-8978c9a10875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## calculating the number of songs labeled per each class\n",
    "## just some general info, not necessary for the evaluation\n",
    "\n",
    "class_scores = {0 : 0, 1 : 0, 2 : 0, 3 : 0,\n",
    "                   4 : 0, 5 : 0, 6 : 0, 7 : 0, 8 : 0}\n",
    "\n",
    "for song in dataset:\n",
    "    max_class = 0\n",
    "    highest_score = 0\n",
    "    for key, value in song['total_class_score'].items():\n",
    "        if highest_score < value:\n",
    "            max_class = class_to_int[key]\n",
    "            highest_score = value\n",
    "    class_scores[max_class] += 1\n",
    "\n",
    "## convert\n",
    "class_scores = {int_to_class[k]: v for k, v in class_scores.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
